# Construct X-Y axes
Consider the following graph

# Show labels
On the x-axis we have some hyperparameter's value, maybe it's the maximum depth of a decision tree.
And on the y-axis we have the error, maybe RMSE or classification error

# Rename to model complexity
And what hyperparameters typically control is the model complexity, so let's assume that this hyperparameter gets more complex as it goes up.

# Rename back
So higher values mean more complexity.

# Show training error
What that means is that as we increase the hyperparameter, we're going to see training error go down.

# Show validation error
But, what we really care about is not the training error -- how the model performs on data it's already seen -- but validation error -- how it performs on unseen data, and this curve will look different.

# Show overfitting and underfitting
In particular, we talk about two scenarios.

# Show error arrows for overfitting
When the model's complexity is too high, it will learn its training data really well, so it will have lower training error, but it overfits to those patters in the training data, and so it performs worse on unseen validation data.

# Show label for overfitting
We call that overfitting.

# Show error arrows for underfitting
On the otherhand, if the model is not complex enough, it won't even perform well on the training data, and so of course it also performs poorly on the validation data.

# Show label for underfitting
We call that underfitting.

# Hide overfitting and underfitting and training error
Our goal is to find the sweet spot between these two.

# Animate a minimum-finding point
That means finding the hyperparameter value that minimizes our validation error, or at least gets close to minimizing it.

# Fade out the minimum point
But how do we actually find that best hyperparameter value?

# Fade out training error plot
To do so, we really don't care about the training error - we can ignore it and focus on the validation error.

# Show validation samples
In reality, when we're tuning our hyperparameters, we can only test a discrete number of hyperparameter values, each with a corresponding validation error.

# Semi-hide validation error graph
So, we don't get a beautiful smooth curve, just a few specific points.

# Highlight an individual point
As a reminder, each one of these points has to be individually computed

# Fade out the rect
# Show the model creation
by training and testing our model, which can take a while.

# Dashed line from point to value on x-axis
First we choose a hyperparameter value

# Highlight the value on the x-axis
which corresponds to a value on our x-axis.

# Show the model hyperparameter
And we fix that hyperparameter value before training our model.

# Show training data
# Show the input data label
We then set the hyperparameter value to 4, and train our model with training data, which includes features (X) and labels (y).
The hyperparameter -- let's say the maximum depth of a decision tree -- is fixed throughout the training process.

# Arrow from validation data to model
Once the model is trained, we evaluate its performance using the validation dataset, which it hasn't seen before. We ask the model to make predictions based on the features, X, of the validation dataset.

# Show the prediction arrow and label
# Arrow from model to predictions
And it makes predictions, y'.

# Error label
We can then calculate the model's validation error when trained with that hyperparameter.

# Highlight the error value
# Move the error value to it's value position
That error value corresponds to the y-axis of our chart here.

# Dashed line from error value to x-axis
So to define a point in our validation error curve, we take the hyperparameter value as the x-value, in this case 4, and the error value as the y-value, in this case 0.33.

# Fade out the labels
# Remove specific point highlights
Remember out goal is to find the hyperparameter value that produces the lowest validation error, or error on unseen data. To do that, we have to try a few more values.

# Update HP value
For example, we might try setting the hyperparameter to 6. Remember, we have to retrain our model with the training data, since changing the hyperparameter creates a different model. If we're increasing the maximum depth of a decision tree, we would get a bigger, more complex decision tree.

# Update error value
We then reevaluate the model using the validation data to calculate an updated validation error; in this case 0.31.

# Move coordinates
Together, these values, 6 and 0.31, form another point on our validation error curve.

# Update HP value again
We might then try another value, say 8, and retrain our model with a hyperparameter value of 8.

# Update error value again
# This gives us a new validation error, 0.33.

# Move coordinates again
Which defines a new point in our validation curve.

# Fade out most things
# Highlight the minimum point
In the end, we take the hyperparameter value that produced the lowest error, in this case 6.
Note that this may not be the absolute lowest possible validation error, since we didn't test every possible point. So the choice of points we sample is important.